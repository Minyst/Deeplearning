{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![KakaoTalk_20230803_163633393.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "nCpCBLDjGyK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png?20211003060202\" height=\"100\">"
      ],
      "metadata": {
        "id": "DBnot-qmHuOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "8To54sznUcPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iL4iVhwMUdpl",
        "outputId": "d2143fba-e697-4144-f31c-0d48ae92b683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.0+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow vs. Pytorch\n",
        "\n",
        "1. 계산 그래프:\n",
        "\n",
        "    * TensorFlow: 정적 계산 그래프\n",
        "    * PyTorch: 동적 계산 그래프\n",
        "\n",
        "2. 사용자 경험:\n",
        "\n",
        "    * TensorFlow: 풍부한 생태계와 도구, 정적 그래프의 최적화\n",
        "    * PyTorch: 직관적이고 Pythonic한 코드, 동적 그래프의 유연성\n",
        "\n",
        "3. 디버깅 및 개발 속도:\n",
        "\n",
        "    * TensorFlow: 그래프를 먼저 정의하고 실행하므로 디버깅이 어렵고 시간이 더 소요될 수 있음\n",
        "    * PyTorch: 실행 시점에 그래프가 정의되므로 디버깅과 개발이 더 용이하고 빠름\n",
        "\n",
        "4. 생태계 및 확장성:\n",
        "\n",
        "    * TensorFlow: 모델 서빙, 모바일 배포 등 다양한 도구와의 통합이 우수\n",
        "    * PyTorch: 연구 및 프로토타이핑에 최적화"
      ],
      "metadata": {
        "id": "tRYX9BLQJWCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch 신경망 기초"
      ],
      "metadata": {
        "id": "toSXsYN5FNt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "print(net)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CwtsvDqGxXS",
        "outputId": "e3f30bea-302e-492f-eb10-7b1727777901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch 모델 학습"
      ],
      "metadata": {
        "id": "u0dUThreH_aM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> PyTorch의 torchvision 패키지를 사용하여 MNIST 데이터셋을 로드합니다.\n"
      ],
      "metadata": {
        "id": "P3mtWWY9IBMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "i5fIeFQcH7DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,))]\n",
        "    )"
      ],
      "metadata": {
        "id": "fT7jhzVYIDo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "XM9hGpOOIEpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "UZ3t-lGVIFHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 손실 함수와 옵티마이저를 정의합니다.\n"
      ],
      "metadata": {
        "id": "_Up8xCIiIMUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "g_4HTdl-IHpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "x1nT2LNoISl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 루프\n",
        "for epoch in range(2):  # 데이터셋을 여러 번 반복 학습\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # 입력 데이터\n",
        "        inputs, labels = data\n",
        "\n",
        "        # 옵티마이저 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 순전파 + 역전파 + 최적화\n",
        "        outputs = net(inputs.view(-1, 28*28))\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 통계 출력\n",
        "        running_loss += loss.item()\n",
        "        # if i % 100 == 99:    # 100 미니배치마다 출력\n",
        "        if (i+1) % 100 == 0:    # 100 미니배치마다 출력\n",
        "            print('epoch: {:<5d} iteration: {:<5d} loss: {:<.3f}'.format(epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7TS5TKdIR-l",
        "outputId": "c54e27de-7eb5-415b-970c-ac8a79cc0349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1     iteration: 100   loss: 1.381\n",
            "epoch: 1     iteration: 200   loss: 0.539\n",
            "epoch: 1     iteration: 300   loss: 0.448\n",
            "epoch: 1     iteration: 400   loss: 0.422\n",
            "epoch: 1     iteration: 500   loss: 0.348\n",
            "epoch: 1     iteration: 600   loss: 0.343\n",
            "epoch: 1     iteration: 700   loss: 0.310\n",
            "epoch: 1     iteration: 800   loss: 0.284\n",
            "epoch: 1     iteration: 900   loss: 0.271\n",
            "epoch: 1     iteration: 1000  loss: 0.249\n",
            "epoch: 1     iteration: 1100  loss: 0.267\n",
            "epoch: 1     iteration: 1200  loss: 0.264\n",
            "epoch: 1     iteration: 1300  loss: 0.216\n",
            "epoch: 1     iteration: 1400  loss: 0.230\n",
            "epoch: 1     iteration: 1500  loss: 0.250\n",
            "epoch: 1     iteration: 1600  loss: 0.222\n",
            "epoch: 1     iteration: 1700  loss: 0.194\n",
            "epoch: 1     iteration: 1800  loss: 0.208\n",
            "epoch: 2     iteration: 100   loss: 0.184\n",
            "epoch: 2     iteration: 200   loss: 0.170\n",
            "epoch: 2     iteration: 300   loss: 0.181\n",
            "epoch: 2     iteration: 400   loss: 0.191\n",
            "epoch: 2     iteration: 500   loss: 0.168\n",
            "epoch: 2     iteration: 600   loss: 0.160\n",
            "epoch: 2     iteration: 700   loss: 0.167\n",
            "epoch: 2     iteration: 800   loss: 0.159\n",
            "epoch: 2     iteration: 900   loss: 0.160\n",
            "epoch: 2     iteration: 1000  loss: 0.155\n",
            "epoch: 2     iteration: 1100  loss: 0.162\n",
            "epoch: 2     iteration: 1200  loss: 0.143\n",
            "epoch: 2     iteration: 1300  loss: 0.150\n",
            "epoch: 2     iteration: 1400  loss: 0.161\n",
            "epoch: 2     iteration: 1500  loss: 0.158\n",
            "epoch: 2     iteration: 1600  loss: 0.146\n",
            "epoch: 2     iteration: 1700  loss: 0.129\n",
            "epoch: 2     iteration: 1800  loss: 0.150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "rbOf7cAAaRUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 중급 신경망 구조: Convolutional Neural Network (CNN)"
      ],
      "metadata": {
        "id": "SlEZ4r3fKK82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* CNN 구조\n",
        "    > Conv2d 레이어:\n",
        "        이미지의 공간적 특징을 추출합니다.  \n",
        "    > MaxPool2d 레이어:\n",
        "        특징 맵의 크기를 줄여 계산량을 줄이고 중요한 특징을 강조합니다.  \n",
        "    > Fully Connected 레이어:\n",
        "        최종 분류를 수행합니다.  "
      ],
      "metadata": {
        "id": "pzpRee6qKNSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "28*28 --> 14*14 --> 7*7*64"
      ],
      "metadata": {
        "id": "vnJAs-tGYmOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = CNN()\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBfHyF51KH18",
        "outputId": "a7f26725-0686-40c1-af3d-af2811bdca3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(2):  # 2 epochs\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()  # Gradient 초기화\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # 매 100 미니배치마다 출력\n",
        "            print('epoch: {:<5d} iteration: {:<5d} loss: {:<.3f}'.format(epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgToqg3kKYLF",
        "outputId": "ef11238d-f5c4-4886-9399-4d1ff69739a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1     iteration: 100   loss: 1.281\n",
            "epoch: 1     iteration: 200   loss: 0.331\n",
            "epoch: 1     iteration: 300   loss: 0.167\n",
            "epoch: 1     iteration: 400   loss: 0.158\n",
            "epoch: 1     iteration: 500   loss: 0.131\n",
            "epoch: 1     iteration: 600   loss: 0.110\n",
            "epoch: 1     iteration: 700   loss: 0.092\n",
            "epoch: 1     iteration: 800   loss: 0.092\n",
            "epoch: 1     iteration: 900   loss: 0.104\n",
            "epoch: 1     iteration: 1000  loss: 0.084\n",
            "epoch: 1     iteration: 1100  loss: 0.071\n",
            "epoch: 1     iteration: 1200  loss: 0.087\n",
            "epoch: 1     iteration: 1300  loss: 0.074\n",
            "epoch: 1     iteration: 1400  loss: 0.065\n",
            "epoch: 1     iteration: 1500  loss: 0.053\n",
            "epoch: 1     iteration: 1600  loss: 0.064\n",
            "epoch: 1     iteration: 1700  loss: 0.062\n",
            "epoch: 1     iteration: 1800  loss: 0.056\n",
            "epoch: 2     iteration: 100   loss: 0.048\n",
            "epoch: 2     iteration: 200   loss: 0.055\n",
            "epoch: 2     iteration: 300   loss: 0.036\n",
            "epoch: 2     iteration: 400   loss: 0.054\n",
            "epoch: 2     iteration: 500   loss: 0.048\n",
            "epoch: 2     iteration: 600   loss: 0.042\n",
            "epoch: 2     iteration: 700   loss: 0.045\n",
            "epoch: 2     iteration: 800   loss: 0.045\n",
            "epoch: 2     iteration: 900   loss: 0.055\n",
            "epoch: 2     iteration: 1000  loss: 0.044\n",
            "epoch: 2     iteration: 1100  loss: 0.032\n",
            "epoch: 2     iteration: 1200  loss: 0.058\n",
            "epoch: 2     iteration: 1300  loss: 0.042\n",
            "epoch: 2     iteration: 1400  loss: 0.047\n",
            "epoch: 2     iteration: 1500  loss: 0.042\n",
            "epoch: 2     iteration: 1600  loss: 0.040\n",
            "epoch: 2     iteration: 1700  loss: 0.047\n",
            "epoch: 2     iteration: 1800  loss: 0.040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 학습된 모델을 평가합니다."
      ],
      "metadata": {
        "id": "7gYyOURmKnJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for data in testloader:\n",
        "\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy {:>.2%}'.format(correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV6feFhJKeyQ",
        "outputId": "fc5b32da-6f80-4ee9-e0a9-ab2566ea321d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 99.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.max(outputs.data, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcAQAjkRcHzT",
        "outputId": "468d0948-178e-45d4-cb96-568fd6520a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(\n",
              "values=tensor([10.9882, 11.8064, 16.2925, 17.5105, 16.5676, 16.2745, 13.2152, 13.7255,\n",
              "        13.2135, 11.5183, 10.7652, 15.5486, 12.4540, 17.3375, 14.3024, 13.4815]),\n",
              "indices=tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3tJBkfocXCR",
        "outputId": "5489e9f6-3b5c-4d13-94d6-2d2239dd049c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.5207, 10.9882, -2.9531, -4.6916,  1.8544, -2.4834, -1.3044,  0.8838,\n",
              "        -0.8338, -0.5849])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "F0ZectBoaSUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN 모델에 Dropout 및 Batch Normalization 추가\n"
      ],
      "metadata": {
        "id": "ZjXhtFSDNu-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
        "        x = self.dropout(F.relu(self.bn3(self.fc1(x))))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = EnhancedCNN()\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzM5aGVxK6y_",
        "outputId": "ddaa472e-4381-486c-f3a4-f277ee2cf24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EnhancedCNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "def train_model(net, trainloader, criterion, optimizer, epoch):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()  # Gradient 초기화\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    avg_training_loss = running_loss / len(trainloader)\n",
        "    print(f\"Epoch [{epoch+1}], Training loss: {avg_training_loss:.3f}\")\n",
        "\n",
        "def evaluate_model(net, testloader, criterion):\n",
        "    net.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_test_loss = running_loss / len(testloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_test_loss, accuracy\n"
      ],
      "metadata": {
        "id": "q4qgZPO5Nxzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> PyTorch는 train()과 eval() 메서드를 사용하여 모델이 학습 모드인지 평가 모드인지 지정합니다.\n",
        "\n",
        "- model.train(): 학습 모드로 설정합니다. Dropout 및 Batch Normalization이 활성화됩니다.\n",
        "- model.eval(): 평가 모드로 설정합니다. Dropout이 비활성화되고, Batch Normalization이 추정된 평균 및 분산을 사용합니다."
      ],
      "metadata": {
        "id": "_G6VafVsO0R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):  # 2 epochs\n",
        "    train_model(net, trainloader, criterion, optimizer, epoch)\n",
        "    test_loss, accuracy = evaluate_model(net, testloader, criterion)\n",
        "    print(f\"Test loss: {test_loss:.3f}, Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyFvHC7fOhwa",
        "outputId": "704f3cd7-c42b-4842-ddd6-304bb6b46bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1], Training loss: 0.117\n",
            "Test loss: 0.040, Accuracy: 98.77%\n",
            "Epoch [2], Training loss: 0.051\n",
            "Test loss: 0.027, Accuracy: 99.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "_KReDw7SaVXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch에서 사전 정의된 모델 불러오기\n"
      ],
      "metadata": {
        "id": "rmqf9Q0iSGFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/vision/stable/models.html"
      ],
      "metadata": {
        "id": "sYwFWdH3apGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* AlexNet\n",
        "* ConvNeXt\n",
        "* DenseNet\n",
        "* EfficientNet\n",
        "* EfficientNetV2\n",
        "* GoogLeNet\n",
        "* Inception V3\n",
        "* MaxVit\n",
        "* MNASNet\n",
        "* MobileNet V2\n",
        "* MobileNet V3\n",
        "* RegNet\n",
        "* ResNet\n",
        "* ResNeXt\n",
        "* ShuffleNet V2\n",
        "* SqueezeNet\n",
        "* SwinTransformer\n",
        "* VGG\n",
        "* VisionTransformer\n",
        "* Wide ResNet\n"
      ],
      "metadata": {
        "id": "6e2GNELyasE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "0iOXMpCxbGU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_html(\"https://pytorch.org/vision/stable/models.html\")[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "QNtWiRz5bJce",
        "outputId": "f65c3971-8506-4e00-bf6d-6f6a11f71fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     Weight   Acc@1   Acc@5  Params  GFLOPS  \\\n",
              "0             AlexNet_Weights.IMAGENET1K_V1  56.522  79.066   61.1M    0.71   \n",
              "1       ConvNeXt_Base_Weights.IMAGENET1K_V1  84.062  96.870   88.6M   15.36   \n",
              "2      ConvNeXt_Large_Weights.IMAGENET1K_V1  84.414  96.976  197.8M   34.36   \n",
              "3      ConvNeXt_Small_Weights.IMAGENET1K_V1  83.616  96.650   50.2M    8.68   \n",
              "4       ConvNeXt_Tiny_Weights.IMAGENET1K_V1  82.520  96.146   28.6M    4.46   \n",
              "..                                      ...     ...     ...     ...     ...   \n",
              "110          ViT_L_32_Weights.IMAGENET1K_V1  76.972  93.070  306.5M   15.38   \n",
              "111  Wide_ResNet101_2_Weights.IMAGENET1K_V1  78.848  94.284  126.9M   22.75   \n",
              "112  Wide_ResNet101_2_Weights.IMAGENET1K_V2  82.510  96.020  126.9M   22.75   \n",
              "113   Wide_ResNet50_2_Weights.IMAGENET1K_V1  78.468  94.086   68.9M   11.40   \n",
              "114   Wide_ResNet50_2_Weights.IMAGENET1K_V2  81.602  95.758   68.9M   11.40   \n",
              "\n",
              "    Recipe  \n",
              "0     link  \n",
              "1     link  \n",
              "2     link  \n",
              "3     link  \n",
              "4     link  \n",
              "..     ...  \n",
              "110   link  \n",
              "111   link  \n",
              "112   link  \n",
              "113   link  \n",
              "114   link  \n",
              "\n",
              "[115 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7cf077b7-9e2e-4499-95b9-5170b8d16811\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acc@1</th>\n",
              "      <th>Acc@5</th>\n",
              "      <th>Params</th>\n",
              "      <th>GFLOPS</th>\n",
              "      <th>Recipe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AlexNet_Weights.IMAGENET1K_V1</td>\n",
              "      <td>56.522</td>\n",
              "      <td>79.066</td>\n",
              "      <td>61.1M</td>\n",
              "      <td>0.71</td>\n",
              "      <td>link</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ConvNeXt_Base_Weights.IMAGENET1K_V1</td>\n",
              "      <td>84.062</td>\n",
              "      <td>96.870</td>\n",
              "      <td>88.6M</td>\n",
              "      <td>15.36</td>\n",
              "      <td>link</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ConvNeXt_Large_Weights.IMAGENET1K_V1</td>\n",
              "      <td>84.414</td>\n",
              "      <td>96.976</td>\n",
              "      <td>197.8M</td>\n",
              "      <td>34.36</td>\n",
              "      <td>link</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ConvNeXt_Small_Weights.IMAGENET1K_V1</td>\n",
              "      <td>83.616</td>\n",
              "      <td>96.650</td>\n",
              "      <td>50.2M</td>\n",
              "      <td>8.68</td>\n",
              "      <td>link</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ConvNeXt_Tiny_Weights.IMAGENET1K_V1</td>\n",
              "      <td>82.520</td>\n",
              "      <td>96.146</td>\n",
              "      <td>28.6M</td>\n",
              "      <td>4.46</td>\n",
              "      <td>link</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>ViT_L_32_Weights.IMAGENET1K_V1</td>\n",
              "      <td>76.972</td>\n",
              "      <td>93.070</td>\n",
              "      <td>306.5M</td>\n",
              "      <td>15.38</td>\n",
              "      <td>link</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Wide_ResNet101_2_Weights.IMAGENET1K_V1</td>\n",
              "      <td>78.848</td>\n",
              "      <td>94.284</td>\n",
              "      <td>126.9M</td>\n",
              "      <td>22.75</td>\n",
              "      <td>link</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>Wide_ResNet101_2_Weights.IMAGENET1K_V2</td>\n",
              "      <td>82.510</td>\n",
              "      <td>96.020</td>\n",
              "      <td>126.9M</td>\n",
              "      <td>22.75</td>\n",
              "      <td>link</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Wide_ResNet50_2_Weights.IMAGENET1K_V1</td>\n",
              "      <td>78.468</td>\n",
              "      <td>94.086</td>\n",
              "      <td>68.9M</td>\n",
              "      <td>11.40</td>\n",
              "      <td>link</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>Wide_ResNet50_2_Weights.IMAGENET1K_V2</td>\n",
              "      <td>81.602</td>\n",
              "      <td>95.758</td>\n",
              "      <td>68.9M</td>\n",
              "      <td>11.40</td>\n",
              "      <td>link</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>115 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7cf077b7-9e2e-4499-95b9-5170b8d16811')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7cf077b7-9e2e-4499-95b9-5170b8d16811 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7cf077b7-9e2e-4499-95b9-5170b8d16811');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f470d1b4-58a5-4628-9821-febb3480a2ba\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f470d1b4-58a5-4628-9821-febb3480a2ba')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f470d1b4-58a5-4628-9821-febb3480a2ba button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 115,\n  \"fields\": [\n    {\n      \"column\": \"Weight\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 115,\n        \"samples\": [\n          \"ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1\",\n          \"ConvNeXt_Tiny_Weights.IMAGENET1K_V1\",\n          \"RegNet_X_3_2GF_Weights.IMAGENET1K_V2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Acc@1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.235458745149328,\n        \"min\": 56.522,\n        \"max\": 88.552,\n        \"num_unique_values\": 113,\n        \"samples\": [\n          60.552,\n          82.52,\n          81.196\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Acc@5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.664256722073394,\n        \"min\": 79.066,\n        \"max\": 98.694,\n        \"num_unique_values\": 113,\n        \"samples\": [\n          88.316,\n          96.146,\n          95.43\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Params\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 77,\n        \"samples\": [\n          \"28.6M\",\n          \"39.6M\",\n          \"7.8M\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GFLOPS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 106.5324426798039,\n        \"min\": 0.04,\n        \"max\": 1016.72,\n        \"num_unique_values\": 80,\n        \"samples\": [\n          15.94,\n          0.71,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recipe\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"link\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"table-weights docutils align-default\">\n",
        "<colgroup>\n",
        "<col style=\"width: 57%\">\n",
        "<col style=\"width: 9%\">\n",
        "<col style=\"width: 9%\">\n",
        "<col style=\"width: 9%\">\n",
        "<col style=\"width: 9%\">\n",
        "<col style=\"width: 5%\">\n",
        "</colgroup>\n",
        "<thead>\n",
        "<tr class=\"row-odd\"><th class=\"head\"><p><strong>Weight</strong></p></th>\n",
        "<th class=\"head\"><p><strong>Acc@1</strong></p></th>\n",
        "<th class=\"head\"><p><strong>Acc@5</strong></p></th>\n",
        "<th class=\"head\"><p><strong>Params</strong></p></th>\n",
        "<th class=\"head\"><p><strong>GFLOPS</strong></p></th>\n",
        "<th class=\"head\"><p><strong>Recipe</strong></p></th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.alexnet.html#torchvision.models.AlexNet_Weights\" title=\"torchvision.models.AlexNet_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">AlexNet_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>56.522</p></td>\n",
        "<td><p>79.066</p></td>\n",
        "<td><p>61.1M</p></td>\n",
        "<td><p>0.71</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.convnext_base.html#torchvision.models.ConvNeXt_Base_Weights\" title=\"torchvision.models.ConvNeXt_Base_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ConvNeXt_Base_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>84.062</p></td>\n",
        "<td><p>96.87</p></td>\n",
        "<td><p>88.6M</p></td>\n",
        "<td><p>15.36</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#convnext\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.convnext_large.html#torchvision.models.ConvNeXt_Large_Weights\" title=\"torchvision.models.ConvNeXt_Large_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ConvNeXt_Large_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>84.414</p></td>\n",
        "<td><p>96.976</p></td>\n",
        "<td><p>197.8M</p></td>\n",
        "<td><p>34.36</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#convnext\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.convnext_small.html#torchvision.models.ConvNeXt_Small_Weights\" title=\"torchvision.models.ConvNeXt_Small_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ConvNeXt_Small_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>83.616</p></td>\n",
        "<td><p>96.65</p></td>\n",
        "<td><p>50.2M</p></td>\n",
        "<td><p>8.68</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#convnext\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.convnext_tiny.html#torchvision.models.ConvNeXt_Tiny_Weights\" title=\"torchvision.models.ConvNeXt_Tiny_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ConvNeXt_Tiny_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>82.52</p></td>\n",
        "<td><p>96.146</p></td>\n",
        "<td><p>28.6M</p></td>\n",
        "<td><p>4.46</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#convnext\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.densenet121.html#torchvision.models.DenseNet121_Weights\" title=\"torchvision.models.DenseNet121_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DenseNet121_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>74.434</p></td>\n",
        "<td><p>91.972</p></td>\n",
        "<td><p>8.0M</p></td>\n",
        "<td><p>2.83</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/116\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.densenet161.html#torchvision.models.DenseNet161_Weights\" title=\"torchvision.models.DenseNet161_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DenseNet161_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>77.138</p></td>\n",
        "<td><p>93.56</p></td>\n",
        "<td><p>28.7M</p></td>\n",
        "<td><p>7.73</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/116\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.densenet169.html#torchvision.models.DenseNet169_Weights\" title=\"torchvision.models.DenseNet169_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DenseNet169_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>75.6</p></td>\n",
        "<td><p>92.806</p></td>\n",
        "<td><p>14.1M</p></td>\n",
        "<td><p>3.36</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/116\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.densenet201.html#torchvision.models.DenseNet201_Weights\" title=\"torchvision.models.DenseNet201_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DenseNet201_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>76.896</p></td>\n",
        "<td><p>93.37</p></td>\n",
        "<td><p>20.0M</p></td>\n",
        "<td><p>4.29</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/116\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.EfficientNet_B0_Weights\" title=\"torchvision.models.EfficientNet_B0_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_B0_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>77.692</p></td>\n",
        "<td><p>93.532</p></td>\n",
        "<td><p>5.3M</p></td>\n",
        "<td><p>0.39</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_b1.html#torchvision.models.EfficientNet_B1_Weights\" title=\"torchvision.models.EfficientNet_B1_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_B1_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>78.642</p></td>\n",
        "<td><p>94.186</p></td>\n",
        "<td><p>7.8M</p></td>\n",
        "<td><p>0.69</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_b1.html#torchvision.models.EfficientNet_B1_Weights\" title=\"torchvision.models.EfficientNet_B1_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_B1_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>79.838</p></td>\n",
        "<td><p>94.934</p></td>\n",
        "<td><p>7.8M</p></td>\n",
        "<td><p>0.69</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe-with-lr-wd-crop-tuning\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights\" title=\"torchvision.models.EfficientNet_B2_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_B2_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>80.608</p></td>\n",
        "<td><p>95.31</p></td>\n",
        "<td><p>9.1M</p></td>\n",
        "<td><p>1.09</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_b3.html#torchvision.models.EfficientNet_B3_Weights\" title=\"torchvision.models.EfficientNet_B3_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_B3_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>82.008</p></td>\n",
        "<td><p>96.054</p></td>\n",
        "<td><p>12.2M</p></td>\n",
        "<td><p>1.83</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_b4.html#torchvision.models.EfficientNet_B4_Weights\" title=\"torchvision.models.EfficientNet_B4_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_B4_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>83.384</p></td>\n",
        "<td><p>96.594</p></td>\n",
        "<td><p>19.3M</p></td>\n",
        "<td><p>4.39</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_b5.html#torchvision.models.EfficientNet_B5_Weights\" title=\"torchvision.models.EfficientNet_B5_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_B5_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>83.444</p></td>\n",
        "<td><p>96.628</p></td>\n",
        "<td><p>30.4M</p></td>\n",
        "<td><p>10.27</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_b6.html#torchvision.models.EfficientNet_B6_Weights\" title=\"torchvision.models.EfficientNet_B6_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_B6_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>84.008</p></td>\n",
        "<td><p>96.916</p></td>\n",
        "<td><p>43.0M</p></td>\n",
        "<td><p>19.07</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_b7.html#torchvision.models.EfficientNet_B7_Weights\" title=\"torchvision.models.EfficientNet_B7_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_B7_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>84.122</p></td>\n",
        "<td><p>96.908</p></td>\n",
        "<td><p>66.3M</p></td>\n",
        "<td><p>37.75</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_v2_l.html#torchvision.models.EfficientNet_V2_L_Weights\" title=\"torchvision.models.EfficientNet_V2_L_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_V2_L_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>85.808</p></td>\n",
        "<td><p>97.788</p></td>\n",
        "<td><p>118.5M</p></td>\n",
        "<td><p>56.08</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_v2_m.html#torchvision.models.EfficientNet_V2_M_Weights\" title=\"torchvision.models.EfficientNet_V2_M_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_V2_M_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>85.112</p></td>\n",
        "<td><p>97.156</p></td>\n",
        "<td><p>54.1M</p></td>\n",
        "<td><p>24.58</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.efficientnet_v2_s.html#torchvision.models.EfficientNet_V2_S_Weights\" title=\"torchvision.models.EfficientNet_V2_S_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">EfficientNet_V2_S_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>84.228</p></td>\n",
        "<td><p>96.878</p></td>\n",
        "<td><p>21.5M</p></td>\n",
        "<td><p>8.37</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.googlenet.html#torchvision.models.GoogLeNet_Weights\" title=\"torchvision.models.GoogLeNet_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">GoogLeNet_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>69.778</p></td>\n",
        "<td><p>89.53</p></td>\n",
        "<td><p>6.6M</p></td>\n",
        "<td><p>1.5</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#googlenet\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.inception_v3.html#torchvision.models.Inception_V3_Weights\" title=\"torchvision.models.Inception_V3_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Inception_V3_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>77.294</p></td>\n",
        "<td><p>93.45</p></td>\n",
        "<td><p>27.2M</p></td>\n",
        "<td><p>5.71</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#inception-v3\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.mnasnet0_5.html#torchvision.models.MNASNet0_5_Weights\" title=\"torchvision.models.MNASNet0_5_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MNASNet0_5_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>67.734</p></td>\n",
        "<td><p>87.49</p></td>\n",
        "<td><p>2.2M</p></td>\n",
        "<td><p>0.1</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/1e100/mnasnet_trainer\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.mnasnet0_75.html#torchvision.models.MNASNet0_75_Weights\" title=\"torchvision.models.MNASNet0_75_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MNASNet0_75_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>71.18</p></td>\n",
        "<td><p>90.496</p></td>\n",
        "<td><p>3.2M</p></td>\n",
        "<td><p>0.21</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/6019\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.mnasnet1_0.html#torchvision.models.MNASNet1_0_Weights\" title=\"torchvision.models.MNASNet1_0_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MNASNet1_0_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>73.456</p></td>\n",
        "<td><p>91.51</p></td>\n",
        "<td><p>4.4M</p></td>\n",
        "<td><p>0.31</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/1e100/mnasnet_trainer\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.mnasnet1_3.html#torchvision.models.MNASNet1_3_Weights\" title=\"torchvision.models.MNASNet1_3_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MNASNet1_3_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>76.506</p></td>\n",
        "<td><p>93.522</p></td>\n",
        "<td><p>6.3M</p></td>\n",
        "<td><p>0.53</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/6019\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.maxvit_t.html#torchvision.models.MaxVit_T_Weights\" title=\"torchvision.models.MaxVit_T_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MaxVit_T_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>83.7</p></td>\n",
        "<td><p>96.722</p></td>\n",
        "<td><p>30.9M</p></td>\n",
        "<td><p>5.56</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#maxvit\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.MobileNet_V2_Weights\" title=\"torchvision.models.MobileNet_V2_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MobileNet_V2_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>71.878</p></td>\n",
        "<td><p>90.286</p></td>\n",
        "<td><p>3.5M</p></td>\n",
        "<td><p>0.3</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv2\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.MobileNet_V2_Weights\" title=\"torchvision.models.MobileNet_V2_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MobileNet_V2_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>72.154</p></td>\n",
        "<td><p>90.822</p></td>\n",
        "<td><p>3.5M</p></td>\n",
        "<td><p>0.3</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.MobileNet_V3_Large_Weights\" title=\"torchvision.models.MobileNet_V3_Large_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MobileNet_V3_Large_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>74.042</p></td>\n",
        "<td><p>91.34</p></td>\n",
        "<td><p>5.5M</p></td>\n",
        "<td><p>0.22</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv3-large--small\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.MobileNet_V3_Large_Weights\" title=\"torchvision.models.MobileNet_V3_Large_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MobileNet_V3_Large_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>75.274</p></td>\n",
        "<td><p>92.566</p></td>\n",
        "<td><p>5.5M</p></td>\n",
        "<td><p>0.22</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.mobilenet_v3_small.html#torchvision.models.MobileNet_V3_Small_Weights\" title=\"torchvision.models.MobileNet_V3_Small_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MobileNet_V3_Small_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>67.668</p></td>\n",
        "<td><p>87.402</p></td>\n",
        "<td><p>2.5M</p></td>\n",
        "<td><p>0.06</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv3-large--small\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_16gf.html#torchvision.models.RegNet_X_16GF_Weights\" title=\"torchvision.models.RegNet_X_16GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_16GF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>80.058</p></td>\n",
        "<td><p>94.944</p></td>\n",
        "<td><p>54.3M</p></td>\n",
        "<td><p>15.94</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#medium-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_16gf.html#torchvision.models.RegNet_X_16GF_Weights\" title=\"torchvision.models.RegNet_X_16GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_16GF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>82.716</p></td>\n",
        "<td><p>96.196</p></td>\n",
        "<td><p>54.3M</p></td>\n",
        "<td><p>15.94</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_1_6gf.html#torchvision.models.RegNet_X_1_6GF_Weights\" title=\"torchvision.models.RegNet_X_1_6GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_1_6GF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>77.04</p></td>\n",
        "<td><p>93.44</p></td>\n",
        "<td><p>9.2M</p></td>\n",
        "<td><p>1.6</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#small-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_1_6gf.html#torchvision.models.RegNet_X_1_6GF_Weights\" title=\"torchvision.models.RegNet_X_1_6GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_1_6GF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>79.668</p></td>\n",
        "<td><p>94.922</p></td>\n",
        "<td><p>9.2M</p></td>\n",
        "<td><p>1.6</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_32gf.html#torchvision.models.RegNet_X_32GF_Weights\" title=\"torchvision.models.RegNet_X_32GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_32GF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>80.622</p></td>\n",
        "<td><p>95.248</p></td>\n",
        "<td><p>107.8M</p></td>\n",
        "<td><p>31.74</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#large-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_32gf.html#torchvision.models.RegNet_X_32GF_Weights\" title=\"torchvision.models.RegNet_X_32GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_32GF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>83.014</p></td>\n",
        "<td><p>96.288</p></td>\n",
        "<td><p>107.8M</p></td>\n",
        "<td><p>31.74</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_3_2gf.html#torchvision.models.RegNet_X_3_2GF_Weights\" title=\"torchvision.models.RegNet_X_3_2GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_3_2GF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>78.364</p></td>\n",
        "<td><p>93.992</p></td>\n",
        "<td><p>15.3M</p></td>\n",
        "<td><p>3.18</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#medium-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_3_2gf.html#torchvision.models.RegNet_X_3_2GF_Weights\" title=\"torchvision.models.RegNet_X_3_2GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_3_2GF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>81.196</p></td>\n",
        "<td><p>95.43</p></td>\n",
        "<td><p>15.3M</p></td>\n",
        "<td><p>3.18</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_400mf.html#torchvision.models.RegNet_X_400MF_Weights\" title=\"torchvision.models.RegNet_X_400MF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_400MF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>72.834</p></td>\n",
        "<td><p>90.95</p></td>\n",
        "<td><p>5.5M</p></td>\n",
        "<td><p>0.41</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#small-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_400mf.html#torchvision.models.RegNet_X_400MF_Weights\" title=\"torchvision.models.RegNet_X_400MF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_400MF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>74.864</p></td>\n",
        "<td><p>92.322</p></td>\n",
        "<td><p>5.5M</p></td>\n",
        "<td><p>0.41</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_800mf.html#torchvision.models.RegNet_X_800MF_Weights\" title=\"torchvision.models.RegNet_X_800MF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_800MF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>75.212</p></td>\n",
        "<td><p>92.348</p></td>\n",
        "<td><p>7.3M</p></td>\n",
        "<td><p>0.8</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#small-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_800mf.html#torchvision.models.RegNet_X_800MF_Weights\" title=\"torchvision.models.RegNet_X_800MF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_800MF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>77.522</p></td>\n",
        "<td><p>93.826</p></td>\n",
        "<td><p>7.3M</p></td>\n",
        "<td><p>0.8</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_8gf.html#torchvision.models.RegNet_X_8GF_Weights\" title=\"torchvision.models.RegNet_X_8GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_8GF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>79.344</p></td>\n",
        "<td><p>94.686</p></td>\n",
        "<td><p>39.6M</p></td>\n",
        "<td><p>8</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#medium-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_x_8gf.html#torchvision.models.RegNet_X_8GF_Weights\" title=\"torchvision.models.RegNet_X_8GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_X_8GF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>81.682</p></td>\n",
        "<td><p>95.678</p></td>\n",
        "<td><p>39.6M</p></td>\n",
        "<td><p>8</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_128gf.html#torchvision.models.RegNet_Y_128GF_Weights\" title=\"torchvision.models.RegNet_Y_128GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>\n",
        "<td><p>88.228</p></td>\n",
        "<td><p>98.682</p></td>\n",
        "<td><p>644.8M</p></td>\n",
        "<td><p>374.57</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/facebookresearch/SWAG\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_128gf.html#torchvision.models.RegNet_Y_128GF_Weights\" title=\"torchvision.models.RegNet_Y_128GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>\n",
        "<td><p>86.068</p></td>\n",
        "<td><p>97.844</p></td>\n",
        "<td><p>644.8M</p></td>\n",
        "<td><p>127.52</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/5793\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights\" title=\"torchvision.models.RegNet_Y_16GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_16GF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>80.424</p></td>\n",
        "<td><p>95.24</p></td>\n",
        "<td><p>83.6M</p></td>\n",
        "<td><p>15.91</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#large-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights\" title=\"torchvision.models.RegNet_Y_16GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_16GF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>82.886</p></td>\n",
        "<td><p>96.328</p></td>\n",
        "<td><p>83.6M</p></td>\n",
        "<td><p>15.91</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights\" title=\"torchvision.models.RegNet_Y_16GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>\n",
        "<td><p>86.012</p></td>\n",
        "<td><p>98.054</p></td>\n",
        "<td><p>83.6M</p></td>\n",
        "<td><p>46.73</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/facebookresearch/SWAG\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights\" title=\"torchvision.models.RegNet_Y_16GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>\n",
        "<td><p>83.976</p></td>\n",
        "<td><p>97.244</p></td>\n",
        "<td><p>83.6M</p></td>\n",
        "<td><p>15.91</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/5793\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_1_6gf.html#torchvision.models.RegNet_Y_1_6GF_Weights\" title=\"torchvision.models.RegNet_Y_1_6GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_1_6GF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>77.95</p></td>\n",
        "<td><p>93.966</p></td>\n",
        "<td><p>11.2M</p></td>\n",
        "<td><p>1.61</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#small-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_1_6gf.html#torchvision.models.RegNet_Y_1_6GF_Weights\" title=\"torchvision.models.RegNet_Y_1_6GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_1_6GF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>80.876</p></td>\n",
        "<td><p>95.444</p></td>\n",
        "<td><p>11.2M</p></td>\n",
        "<td><p>1.61</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights\" title=\"torchvision.models.RegNet_Y_32GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_32GF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>80.878</p></td>\n",
        "<td><p>95.34</p></td>\n",
        "<td><p>145.0M</p></td>\n",
        "<td><p>32.28</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#large-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights\" title=\"torchvision.models.RegNet_Y_32GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_32GF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>83.368</p></td>\n",
        "<td><p>96.498</p></td>\n",
        "<td><p>145.0M</p></td>\n",
        "<td><p>32.28</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights\" title=\"torchvision.models.RegNet_Y_32GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>\n",
        "<td><p>86.838</p></td>\n",
        "<td><p>98.362</p></td>\n",
        "<td><p>145.0M</p></td>\n",
        "<td><p>94.83</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/facebookresearch/SWAG\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights\" title=\"torchvision.models.RegNet_Y_32GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>\n",
        "<td><p>84.622</p></td>\n",
        "<td><p>97.48</p></td>\n",
        "<td><p>145.0M</p></td>\n",
        "<td><p>32.28</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/5793\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_3_2gf.html#torchvision.models.RegNet_Y_3_2GF_Weights\" title=\"torchvision.models.RegNet_Y_3_2GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_3_2GF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>78.948</p></td>\n",
        "<td><p>94.576</p></td>\n",
        "<td><p>19.4M</p></td>\n",
        "<td><p>3.18</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#medium-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_3_2gf.html#torchvision.models.RegNet_Y_3_2GF_Weights\" title=\"torchvision.models.RegNet_Y_3_2GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_3_2GF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>81.982</p></td>\n",
        "<td><p>95.972</p></td>\n",
        "<td><p>19.4M</p></td>\n",
        "<td><p>3.18</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_400mf.html#torchvision.models.RegNet_Y_400MF_Weights\" title=\"torchvision.models.RegNet_Y_400MF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_400MF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>74.046</p></td>\n",
        "<td><p>91.716</p></td>\n",
        "<td><p>4.3M</p></td>\n",
        "<td><p>0.4</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#small-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_400mf.html#torchvision.models.RegNet_Y_400MF_Weights\" title=\"torchvision.models.RegNet_Y_400MF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_400MF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>75.804</p></td>\n",
        "<td><p>92.742</p></td>\n",
        "<td><p>4.3M</p></td>\n",
        "<td><p>0.4</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_800mf.html#torchvision.models.RegNet_Y_800MF_Weights\" title=\"torchvision.models.RegNet_Y_800MF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_800MF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>76.42</p></td>\n",
        "<td><p>93.136</p></td>\n",
        "<td><p>6.4M</p></td>\n",
        "<td><p>0.83</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#small-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_800mf.html#torchvision.models.RegNet_Y_800MF_Weights\" title=\"torchvision.models.RegNet_Y_800MF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_800MF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>78.828</p></td>\n",
        "<td><p>94.502</p></td>\n",
        "<td><p>6.4M</p></td>\n",
        "<td><p>0.83</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_8gf.html#torchvision.models.RegNet_Y_8GF_Weights\" title=\"torchvision.models.RegNet_Y_8GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_8GF_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>80.032</p></td>\n",
        "<td><p>95.048</p></td>\n",
        "<td><p>39.4M</p></td>\n",
        "<td><p>8.47</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#medium-models\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.regnet_y_8gf.html#torchvision.models.RegNet_Y_8GF_Weights\" title=\"torchvision.models.RegNet_Y_8GF_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RegNet_Y_8GF_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>82.828</p></td>\n",
        "<td><p>96.33</p></td>\n",
        "<td><p>39.4M</p></td>\n",
        "<td><p>8.47</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnext101_32x8d.html#torchvision.models.ResNeXt101_32X8D_Weights\" title=\"torchvision.models.ResNeXt101_32X8D_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNeXt101_32X8D_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>79.312</p></td>\n",
        "<td><p>94.526</p></td>\n",
        "<td><p>88.8M</p></td>\n",
        "<td><p>16.41</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#resnext\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnext101_32x8d.html#torchvision.models.ResNeXt101_32X8D_Weights\" title=\"torchvision.models.ResNeXt101_32X8D_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNeXt101_32X8D_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>82.834</p></td>\n",
        "<td><p>96.228</p></td>\n",
        "<td><p>88.8M</p></td>\n",
        "<td><p>16.41</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnext101_64x4d.html#torchvision.models.ResNeXt101_64X4D_Weights\" title=\"torchvision.models.ResNeXt101_64X4D_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNeXt101_64X4D_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>83.246</p></td>\n",
        "<td><p>96.454</p></td>\n",
        "<td><p>83.5M</p></td>\n",
        "<td><p>15.46</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/5935\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnext50_32x4d.html#torchvision.models.ResNeXt50_32X4D_Weights\" title=\"torchvision.models.ResNeXt50_32X4D_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNeXt50_32X4D_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>77.618</p></td>\n",
        "<td><p>93.698</p></td>\n",
        "<td><p>25.0M</p></td>\n",
        "<td><p>4.23</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#resnext\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnext50_32x4d.html#torchvision.models.ResNeXt50_32X4D_Weights\" title=\"torchvision.models.ResNeXt50_32X4D_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNeXt50_32X4D_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>81.198</p></td>\n",
        "<td><p>95.34</p></td>\n",
        "<td><p>25.0M</p></td>\n",
        "<td><p>4.23</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnet101.html#torchvision.models.ResNet101_Weights\" title=\"torchvision.models.ResNet101_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNet101_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>77.374</p></td>\n",
        "<td><p>93.546</p></td>\n",
        "<td><p>44.5M</p></td>\n",
        "<td><p>7.8</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#resnet\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnet101.html#torchvision.models.ResNet101_Weights\" title=\"torchvision.models.ResNet101_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNet101_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>81.886</p></td>\n",
        "<td><p>95.78</p></td>\n",
        "<td><p>44.5M</p></td>\n",
        "<td><p>7.8</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnet152.html#torchvision.models.ResNet152_Weights\" title=\"torchvision.models.ResNet152_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNet152_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>78.312</p></td>\n",
        "<td><p>94.046</p></td>\n",
        "<td><p>60.2M</p></td>\n",
        "<td><p>11.51</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#resnet\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnet152.html#torchvision.models.ResNet152_Weights\" title=\"torchvision.models.ResNet152_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNet152_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>82.284</p></td>\n",
        "<td><p>96.002</p></td>\n",
        "<td><p>60.2M</p></td>\n",
        "<td><p>11.51</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnet18.html#torchvision.models.ResNet18_Weights\" title=\"torchvision.models.ResNet18_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNet18_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>69.758</p></td>\n",
        "<td><p>89.078</p></td>\n",
        "<td><p>11.7M</p></td>\n",
        "<td><p>1.81</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#resnet\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnet34.html#torchvision.models.ResNet34_Weights\" title=\"torchvision.models.ResNet34_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNet34_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>73.314</p></td>\n",
        "<td><p>91.42</p></td>\n",
        "<td><p>21.8M</p></td>\n",
        "<td><p>3.66</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#resnet\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights\" title=\"torchvision.models.ResNet50_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNet50_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>76.13</p></td>\n",
        "<td><p>92.862</p></td>\n",
        "<td><p>25.6M</p></td>\n",
        "<td><p>4.09</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#resnet\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights\" title=\"torchvision.models.ResNet50_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ResNet50_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>80.858</p></td>\n",
        "<td><p>95.434</p></td>\n",
        "<td><p>25.6M</p></td>\n",
        "<td><p>4.09</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#issuecomment-1013906621\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.shufflenet_v2_x0_5.html#torchvision.models.ShuffleNet_V2_X0_5_Weights\" title=\"torchvision.models.ShuffleNet_V2_X0_5_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>60.552</p></td>\n",
        "<td><p>81.746</p></td>\n",
        "<td><p>1.4M</p></td>\n",
        "<td><p>0.04</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/ericsun99/Shufflenet-v2-Pytorch\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.shufflenet_v2_x1_0.html#torchvision.models.ShuffleNet_V2_X1_0_Weights\" title=\"torchvision.models.ShuffleNet_V2_X1_0_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>69.362</p></td>\n",
        "<td><p>88.316</p></td>\n",
        "<td><p>2.3M</p></td>\n",
        "<td><p>0.14</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/ericsun99/Shufflenet-v2-Pytorch\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.shufflenet_v2_x1_5.html#torchvision.models.ShuffleNet_V2_X1_5_Weights\" title=\"torchvision.models.ShuffleNet_V2_X1_5_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ShuffleNet_V2_X1_5_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>72.996</p></td>\n",
        "<td><p>91.086</p></td>\n",
        "<td><p>3.5M</p></td>\n",
        "<td><p>0.3</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/5906\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.shufflenet_v2_x2_0.html#torchvision.models.ShuffleNet_V2_X2_0_Weights\" title=\"torchvision.models.ShuffleNet_V2_X2_0_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ShuffleNet_V2_X2_0_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>76.23</p></td>\n",
        "<td><p>93.006</p></td>\n",
        "<td><p>7.4M</p></td>\n",
        "<td><p>0.58</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/5906\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.squeezenet1_0.html#torchvision.models.SqueezeNet1_0_Weights\" title=\"torchvision.models.SqueezeNet1_0_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SqueezeNet1_0_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>58.092</p></td>\n",
        "<td><p>80.42</p></td>\n",
        "<td><p>1.2M</p></td>\n",
        "<td><p>0.82</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/49#issuecomment-277560717\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.squeezenet1_1.html#torchvision.models.SqueezeNet1_1_Weights\" title=\"torchvision.models.SqueezeNet1_1_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SqueezeNet1_1_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>58.178</p></td>\n",
        "<td><p>80.624</p></td>\n",
        "<td><p>1.2M</p></td>\n",
        "<td><p>0.35</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/49#issuecomment-277560717\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.swin_b.html#torchvision.models.Swin_B_Weights\" title=\"torchvision.models.Swin_B_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Swin_B_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>83.582</p></td>\n",
        "<td><p>96.64</p></td>\n",
        "<td><p>87.8M</p></td>\n",
        "<td><p>15.43</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.swin_s.html#torchvision.models.Swin_S_Weights\" title=\"torchvision.models.Swin_S_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Swin_S_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>83.196</p></td>\n",
        "<td><p>96.36</p></td>\n",
        "<td><p>49.6M</p></td>\n",
        "<td><p>8.74</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.swin_t.html#torchvision.models.Swin_T_Weights\" title=\"torchvision.models.Swin_T_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Swin_T_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>81.474</p></td>\n",
        "<td><p>95.776</p></td>\n",
        "<td><p>28.3M</p></td>\n",
        "<td><p>4.49</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.swin_v2_b.html#torchvision.models.Swin_V2_B_Weights\" title=\"torchvision.models.Swin_V2_B_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Swin_V2_B_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>84.112</p></td>\n",
        "<td><p>96.864</p></td>\n",
        "<td><p>87.9M</p></td>\n",
        "<td><p>20.32</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.swin_v2_s.html#torchvision.models.Swin_V2_S_Weights\" title=\"torchvision.models.Swin_V2_S_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Swin_V2_S_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>83.712</p></td>\n",
        "<td><p>96.816</p></td>\n",
        "<td><p>49.7M</p></td>\n",
        "<td><p>11.55</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.swin_v2_t.html#torchvision.models.Swin_V2_T_Weights\" title=\"torchvision.models.Swin_V2_T_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Swin_V2_T_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>82.072</p></td>\n",
        "<td><p>96.132</p></td>\n",
        "<td><p>28.4M</p></td>\n",
        "<td><p>5.94</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vgg11_bn.html#torchvision.models.VGG11_BN_Weights\" title=\"torchvision.models.VGG11_BN_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">VGG11_BN_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>70.37</p></td>\n",
        "<td><p>89.81</p></td>\n",
        "<td><p>132.9M</p></td>\n",
        "<td><p>7.61</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vgg11.html#torchvision.models.VGG11_Weights\" title=\"torchvision.models.VGG11_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">VGG11_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>69.02</p></td>\n",
        "<td><p>88.628</p></td>\n",
        "<td><p>132.9M</p></td>\n",
        "<td><p>7.61</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vgg13_bn.html#torchvision.models.VGG13_BN_Weights\" title=\"torchvision.models.VGG13_BN_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">VGG13_BN_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>71.586</p></td>\n",
        "<td><p>90.374</p></td>\n",
        "<td><p>133.1M</p></td>\n",
        "<td><p>11.31</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vgg13.html#torchvision.models.VGG13_Weights\" title=\"torchvision.models.VGG13_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">VGG13_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>69.928</p></td>\n",
        "<td><p>89.246</p></td>\n",
        "<td><p>133.0M</p></td>\n",
        "<td><p>11.31</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vgg16_bn.html#torchvision.models.VGG16_BN_Weights\" title=\"torchvision.models.VGG16_BN_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">VGG16_BN_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>73.36</p></td>\n",
        "<td><p>91.516</p></td>\n",
        "<td><p>138.4M</p></td>\n",
        "<td><p>15.47</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights\" title=\"torchvision.models.VGG16_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">VGG16_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>71.592</p></td>\n",
        "<td><p>90.382</p></td>\n",
        "<td><p>138.4M</p></td>\n",
        "<td><p>15.47</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights\" title=\"torchvision.models.VGG16_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">VGG16_Weights.IMAGENET1K_FEATURES</span></code></a></p></td>\n",
        "<td><p>nan</p></td>\n",
        "<td><p>nan</p></td>\n",
        "<td><p>138.4M</p></td>\n",
        "<td><p>15.47</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/amdegroot/ssd.pytorch#training-ssd\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vgg19_bn.html#torchvision.models.VGG19_BN_Weights\" title=\"torchvision.models.VGG19_BN_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">VGG19_BN_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>74.218</p></td>\n",
        "<td><p>91.842</p></td>\n",
        "<td><p>143.7M</p></td>\n",
        "<td><p>19.63</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vgg19.html#torchvision.models.VGG19_Weights\" title=\"torchvision.models.VGG19_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">VGG19_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>72.376</p></td>\n",
        "<td><p>90.876</p></td>\n",
        "<td><p>143.7M</p></td>\n",
        "<td><p>19.63</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights\" title=\"torchvision.models.ViT_B_16_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ViT_B_16_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>81.072</p></td>\n",
        "<td><p>95.318</p></td>\n",
        "<td><p>86.6M</p></td>\n",
        "<td><p>17.56</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#vit_b_16\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights\" title=\"torchvision.models.ViT_B_16_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>\n",
        "<td><p>85.304</p></td>\n",
        "<td><p>97.65</p></td>\n",
        "<td><p>86.9M</p></td>\n",
        "<td><p>55.48</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/facebookresearch/SWAG\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights\" title=\"torchvision.models.ViT_B_16_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>\n",
        "<td><p>81.886</p></td>\n",
        "<td><p>96.18</p></td>\n",
        "<td><p>86.6M</p></td>\n",
        "<td><p>17.56</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/5793\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vit_b_32.html#torchvision.models.ViT_B_32_Weights\" title=\"torchvision.models.ViT_B_32_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ViT_B_32_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>75.912</p></td>\n",
        "<td><p>92.466</p></td>\n",
        "<td><p>88.2M</p></td>\n",
        "<td><p>4.41</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#vit_b_32\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vit_h_14.html#torchvision.models.ViT_H_14_Weights\" title=\"torchvision.models.ViT_H_14_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>\n",
        "<td><p>88.552</p></td>\n",
        "<td><p>98.694</p></td>\n",
        "<td><p>633.5M</p></td>\n",
        "<td><p>1016.72</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/facebookresearch/SWAG\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vit_h_14.html#torchvision.models.ViT_H_14_Weights\" title=\"torchvision.models.ViT_H_14_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>\n",
        "<td><p>85.708</p></td>\n",
        "<td><p>97.73</p></td>\n",
        "<td><p>632.0M</p></td>\n",
        "<td><p>167.29</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/5793\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights\" title=\"torchvision.models.ViT_L_16_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ViT_L_16_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>79.662</p></td>\n",
        "<td><p>94.638</p></td>\n",
        "<td><p>304.3M</p></td>\n",
        "<td><p>61.55</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#vit_l_16\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights\" title=\"torchvision.models.ViT_L_16_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ViT_L_16_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>\n",
        "<td><p>88.064</p></td>\n",
        "<td><p>98.512</p></td>\n",
        "<td><p>305.2M</p></td>\n",
        "<td><p>361.99</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/facebookresearch/SWAG\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights\" title=\"torchvision.models.ViT_L_16_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>\n",
        "<td><p>85.146</p></td>\n",
        "<td><p>97.422</p></td>\n",
        "<td><p>304.3M</p></td>\n",
        "<td><p>61.55</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/5793\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.vit_l_32.html#torchvision.models.ViT_L_32_Weights\" title=\"torchvision.models.ViT_L_32_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ViT_L_32_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>76.972</p></td>\n",
        "<td><p>93.07</p></td>\n",
        "<td><p>306.5M</p></td>\n",
        "<td><p>15.38</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/tree/main/references/classification#vit_l_32\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.wide_resnet101_2.html#torchvision.models.Wide_ResNet101_2_Weights\" title=\"torchvision.models.Wide_ResNet101_2_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Wide_ResNet101_2_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>78.848</p></td>\n",
        "<td><p>94.284</p></td>\n",
        "<td><p>126.9M</p></td>\n",
        "<td><p>22.75</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/912#issue-445437439\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.wide_resnet101_2.html#torchvision.models.Wide_ResNet101_2_Weights\" title=\"torchvision.models.Wide_ResNet101_2_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Wide_ResNet101_2_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>82.51</p></td>\n",
        "<td><p>96.02</p></td>\n",
        "<td><p>126.9M</p></td>\n",
        "<td><p>22.75</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-odd\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.wide_resnet50_2.html#torchvision.models.Wide_ResNet50_2_Weights\" title=\"torchvision.models.Wide_ResNet50_2_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Wide_ResNet50_2_Weights.IMAGENET1K_V1</span></code></a></p></td>\n",
        "<td><p>78.468</p></td>\n",
        "<td><p>94.086</p></td>\n",
        "<td><p>68.9M</p></td>\n",
        "<td><p>11.4</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/pull/912#issue-445437439\">link</a></p></td>\n",
        "</tr>\n",
        "<tr class=\"row-even\"><td><p><a class=\"reference internal has-code\" href=\"models/generated/torchvision.models.wide_resnet50_2.html#torchvision.models.Wide_ResNet50_2_Weights\" title=\"torchvision.models.Wide_ResNet50_2_Weights\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Wide_ResNet50_2_Weights.IMAGENET1K_V2</span></code></a></p></td>\n",
        "<td><p>81.602</p></td>\n",
        "<td><p>95.758</p></td>\n",
        "<td><p>68.9M</p></td>\n",
        "<td><p>11.4</p></td>\n",
        "<td><p><a class=\"reference external\" href=\"https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres\">link</a></p></td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "XI1b7rmya73C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "XpAq8av_OiV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pbSxVdPmaiDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet-18 모델 불러오기 (사전 훈련된 가중치 사용)\n",
        "resnet18 = models.resnet18(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubf_b5WwSL--",
        "outputId": "3c8fe700-18f5-4086-87c3-02cac1a16979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 95.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 훈련된 모델을 사용하지 않으려면 pretrained=False로 설정\n",
        "# resnet18 = models.resnet18(pretrained=False)"
      ],
      "metadata": {
        "id": "-veiiGl7SJtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 구조 확인\n",
        "print(resnet18)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4iv0r-rSKMl",
        "outputId": "81a5b36c-a0e7-4742-bea0-26961eacff0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "사전 정의된 모델을 불러온 후, 마지막 레이어를 변경하여 특정 작업에 맞게 수정할 수 있습니다.  \n",
        "예를 들어, ResNet-18을 불러와 MNIST 데이터셋에 맞게 출력 클래스를 10개로 수정해 보겠습니다.\n"
      ],
      "metadata": {
        "id": "MhLqX4mwSOo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 마지막 레이어의 출력 뉴런 수를 10으로 수정 (MNIST는 10개의 클래스)\n",
        "num_ftrs = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Linear(num_ftrs, 10)"
      ],
      "metadata": {
        "id": "qLMdYaxSSI2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 구조 확인\n",
        "print(resnet18)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEyLGK4TSSI5",
        "outputId": "be6748ce-dd32-4070-ca8d-5ed1e1353bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 변환 및 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),  # ResNet은 3채널 입력을 해야합니다.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "17MXqBh9TFLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(resnet18.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 학습 함수 정의\n",
        "def train_model(net, trainloader, criterion, optimizer, num_epochs=2):\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:\n",
        "                print('epoch: {:<5d} iteration: {:<5d} loss: {:<.3f}'.format(epoch + 1, i + 1, running_loss / 100))\n",
        "                running_loss = 0.0\n",
        "\n",
        "# 평가 함수 정의\n",
        "def evaluate_model(net, testloader, criterion):\n",
        "    net.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_test_loss = running_loss / len(testloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test loss: {avg_test_loss:.3f}, Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "8Vg27FQcSslA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 및 평가\n",
        "train_model(resnet18, trainloader, criterion, optimizer, num_epochs=2)\n",
        "evaluate_model(resnet18, testloader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHoCuPswS_xa",
        "outputId": "ef90ec97-bf79-4e41-f335-00720a3e2d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1     iteration: 100   loss: 0.774\n",
            "epoch: 1     iteration: 200   loss: 0.239\n",
            "epoch: 1     iteration: 300   loss: 0.172\n",
            "epoch: 1     iteration: 400   loss: 0.158\n",
            "epoch: 1     iteration: 500   loss: 0.147\n",
            "epoch: 1     iteration: 600   loss: 0.128\n",
            "epoch: 1     iteration: 700   loss: 0.116\n",
            "epoch: 1     iteration: 800   loss: 0.108\n",
            "epoch: 1     iteration: 900   loss: 0.081\n",
            "epoch: 1     iteration: 1000  loss: 0.080\n",
            "epoch: 1     iteration: 1100  loss: 0.076\n",
            "epoch: 1     iteration: 1200  loss: 0.078\n",
            "epoch: 1     iteration: 1300  loss: 0.078\n",
            "epoch: 1     iteration: 1400  loss: 0.065\n",
            "epoch: 1     iteration: 1500  loss: 0.074\n",
            "epoch: 1     iteration: 1600  loss: 0.057\n",
            "epoch: 1     iteration: 1700  loss: 0.073\n",
            "epoch: 1     iteration: 1800  loss: 0.064\n",
            "epoch: 2     iteration: 100   loss: 0.047\n",
            "epoch: 2     iteration: 200   loss: 0.053\n",
            "epoch: 2     iteration: 300   loss: 0.043\n",
            "epoch: 2     iteration: 400   loss: 0.037\n",
            "epoch: 2     iteration: 500   loss: 0.043\n",
            "epoch: 2     iteration: 600   loss: 0.042\n",
            "epoch: 2     iteration: 700   loss: 0.047\n",
            "epoch: 2     iteration: 800   loss: 0.041\n",
            "epoch: 2     iteration: 900   loss: 0.043\n",
            "epoch: 2     iteration: 1000  loss: 0.032\n",
            "epoch: 2     iteration: 1100  loss: 0.056\n",
            "epoch: 2     iteration: 1200  loss: 0.040\n",
            "epoch: 2     iteration: 1300  loss: 0.035\n",
            "epoch: 2     iteration: 1400  loss: 0.030\n",
            "epoch: 2     iteration: 1500  loss: 0.031\n",
            "epoch: 2     iteration: 1600  loss: 0.023\n",
            "epoch: 2     iteration: 1700  loss: 0.033\n",
            "epoch: 2     iteration: 1800  loss: 0.028\n",
            "Test loss: 0.027, Accuracy: 99.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSjCnRnETBJS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}